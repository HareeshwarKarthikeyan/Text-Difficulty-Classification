{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49199ac8-c2eb-46dd-9e4a-a2a544fd240a",
   "metadata": {},
   "source": [
    "# FEATURE CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02523c4-ed23-462c-b8e9-7fcaafea0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB,ComplementNB\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from time import time\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929ea94-3fdf-415f-aed5-b6126e93d456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('WikiLarge_Train.csv')\n",
    "test_data = pd.read_csv('WikiLarge_Test.csv')\n",
    "sample_submission = pd.read_csv('sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c67db-0a74-448c-a29f-df2c8d3c19ec",
   "metadata": {},
   "source": [
    "## 1 : Tokenising the text and part of speech tagging\n",
    "- Done using NTLK\n",
    "- The text was divided into tokens\n",
    "- The part of speech for every POS token was found using pos_tag from ntlk\n",
    "- The pos tags of every word in the sentence are stored in the form of a dictionary.So I have changed the dictionary to series so that every part of speech tag becomes a column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05040b-78a1-4119-a32e-a6b0452fbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tic = time()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train_data[\"original_text\"]=train_data[\"original_text\"].str.lower()\n",
    "train_data['tokenized_text'] = train_data[\"original_text\"].apply(tokenizer.tokenize)\n",
    "train_data['pos_tokens'] = train_data['tokenized_text'].apply(nltk.pos_tag)\n",
    "test_data[\"original_text\"]=test_data[\"original_text\"].str.lower()\n",
    "test_data['tokenized_text'] = test_data[\"original_text\"].apply(tokenizer.tokenize)\n",
    "test_data['pos_tokens'] = test_data['tokenized_text'].apply(nltk.pos_tag)\n",
    "# train_data['num_unique']=train_data['tokenized_text'].apply(pd.Series.nunique)\n",
    "toc = time()\n",
    "print(f\"Done in {toc - tic:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d318bd-4779-475e-833d-47b88029e8a3",
   "metadata": {},
   "source": [
    "## Calculating numeric features from plain text\n",
    "- The text was syllabified using the pyhyphen library and then features like \n",
    "    - number of words with one syllable\n",
    "    - number of words with two syllables\n",
    "    - number of words with three syllables\n",
    "    - number of words with four syllables\n",
    "    - number of words with five syllables\n",
    "    - lexical diversity was calculated\n",
    "- We had the pos tag for every word in the input from the previous step.We also calculated the number of words under each POS tag for every input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bcf4c-b171-489c-b0fd-fc76ef7462de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "syllabifier = pyphen.Pyphen(lang='en')\n",
    "#lexical diversity\n",
    "def lexical_diversity(row):\n",
    "#     print(len(set(row['pos_tokens']))/float(len('pos_tokens')))\n",
    "    return len(set(row['pos_tokens']))/float(len('pos_tokens'))\n",
    "\n",
    "#counting the number of words under each tag\n",
    "def tagcounter(row):\n",
    "    return Counter(tag for word, tag in row[\"pos_tokens\"])\n",
    "def sentence(row):\n",
    "    return list(map(syllabify,row[\"tokenized_text\"]))\n",
    "def syllabify(a):\n",
    "    return syllabifier.inserted(a).count('-')+1\n",
    "\n",
    "def one_syllable_count(row):\n",
    "    return row[\"syllabified\"].count(1)\n",
    "def two_syllable_count(row):\n",
    "    return row[\"syllabified\"].count(2)\n",
    "\n",
    "def three_syllable_count(row):\n",
    "#     print(\"-\")\n",
    "    return row[\"syllabified\"].count(3)\n",
    "\n",
    "def four_syllable_count(row):\n",
    "    return row[\"syllabified\"].count(4)\n",
    "\n",
    "def five_syllable_count(row):\n",
    "    return row[\"syllabified\"].count(5)\n",
    "def num(row):\n",
    "    return len(row[\"tokenized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18d7e0-bc18-4d8c-9766-f6522c9b88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_numeric_features(data):\n",
    "    data[\"lexical_diversity\"]=data.apply(lambda row:lexical_diversity(row),axis=1)\n",
    "    data[\"pos_count\"]=data.apply(lambda row:tagcounter(row),axis=1)\n",
    "    verb_cols = [col for col in data.columns if 'verb' in col and \"adverb\" not in col]\n",
    "    noun_cols=[col for col in data.columns if 'noun' in col and \"pronoun\" not in col]\n",
    "    pronoun_cols=[col for col in data.columns if \"pronoun\" in col]\n",
    "    data[\"verb_count\"]=data[verb_cols].sum(axis = 1, skipna = True)\n",
    "    data[\"noun_count\"]=data[noun_cols].sum(axis = 1, skipna = True)\n",
    "    data[\"pronoun_count\"]=data[pronoun_cols].sum(axis = 1, skipna = True)\n",
    "    data[\"syllabified\"]=data.apply(lambda row:sentence(row),axis=1)\n",
    "    data[\"1_syllable_count\"]=data.apply(lambda row:one_syllable_count(row),axis=1)\n",
    "    data[\"2_syllable_count\"]=data.apply(lambda row:two_syllable_count(row),axis=1)\n",
    "    data[\"3_syllable_count\"]=data.apply(lambda row:three_syllable_count(row),axis=1)\n",
    "    data[\"4_syllable_count\"]=data.apply(lambda row:four_syllable_count(row),axis=1)\n",
    "    data[\"5_syllable_count\"]=data.apply(lambda row:five_syllable_count(row),axis=1)\n",
    "    data['length']=len(data['tokenized_text'])\n",
    "    data[\"length\"]=data.apply(lambda row:num(row),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f6794-7ded-45d5-8b7c-f8455e9c2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=calculate_numeric_features(train_data)\n",
    "test_data=calculate_numeric_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09282be-2d77-4555-8c60-4f3e1cbfb9d3",
   "metadata": {},
   "source": [
    "## Calculating features from the part of speech tags\n",
    "- We have renamed the columns in the dataframe for easier understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c8a68-3308-46cb-9d61-97d63728ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_train_data=train_data[\"pos_count\"].apply(pd.Series)\n",
    "pos_counts_test_data=test_data[\"pos_count\"].apply(pd.Series)\n",
    "pos_counts_train_data.rename(columns={\"DT\":\"determiner\",\"CD\":\"numeral\",\\\n",
    "                                                \"CC\":\"conjunction\",\"EX\": \"existential\",\\\n",
    "                                                \"VBZ\": \"verb_present_3s\",\"JJ\":\"ordinal\",\\\n",
    "                                                \"NN\": \"noun_common\",\"WDT\": \"WH_determiner\",\\\n",
    "                                                \"VBN\": \"verb_past_participle\",\\\n",
    "                                                \"CC\": \"conjunction\",\"CD\": \"numeral\",\\\n",
    "                                                \"DT\": \"determiner\",\"IN\":\"preposition\",\"JJR\": \"adjective_comparative\",\\\n",
    "                                                \"JJS\": \"adjective_superlative\",\"LS\": \"list\",\\\n",
    "                                                \"MD\": \"modal_auxiliary\",\"NNP\": \"noun_singular\",\\\n",
    "                                                \"NNS\": \"noun_common_plural\",\"PDT\": \"pre_determiner\",\\\n",
    "                                                \"POS\": \"genitive_marker\",\"PRP\": \"pronoun_personal\",\\\n",
    "                                                \"PRP$\": \"pronoun_possessive\",\\\n",
    "                                                \"RB\": \"adverb\",\"RBR\": \"adverb_comparative\",\\\n",
    "                                                \"RBS\": \"adverb_superlative\",\"RP\": \"particle\",\\\n",
    "                                                \"TO\":\"to_preposition\",\"UH\": \"interjection\",\\\n",
    "                                                \"VB\": \"verb_base\",\"VBD\":\"verb_past\",\\\n",
    "                                                \"VBG\":\"gerund\",\"VBP\": \"verb_present_3s\",\\\n",
    "                                                \"VBZ\": \"verb_present_3s\",\"WP\": \"WH_pronoun\",\n",
    "                                                \"WRB\": \"Wh_adverb\",\"WP$\":\"possessive_wh_pronoun\",\\\n",
    "                            \"FW\":\"foreign word\",\"SYM\":\"symbol\",'NNPS':\"proper_noun\"},inplace=True)\n",
    "pos_counts_test_data.rename(columns={\"DT\":\"determiner\",\"CD\":\"numeral\",\\\n",
    "                                                \"CC\":\"conjunction\",\"EX\": \"existential\",\\\n",
    "                                                \"VBZ\": \"verb_present_3s\",\"JJ\":\"ordinal\",\\\n",
    "                                                \"NN\": \"noun_common\",\"WDT\": \"WH_determiner\",\\\n",
    "                                                \"VBN\": \"verb_past_participle\",\\\n",
    "                                                \"CC\": \"conjunction\",\"CD\": \"numeral\",\\\n",
    "                                                \"DT\": \"determiner\",\"IN\":\"preposition\",\"JJR\": \"adjective_comparative\",\\\n",
    "                                                \"JJS\": \"adjective_superlative\",\"LS\": \"list\",\\\n",
    "                                                \"MD\": \"modal_auxiliary\",\"NNP\": \"noun_singular\",\\\n",
    "                                                \"NNS\": \"noun_common_plural\",\"PDT\": \"pre_determiner\",\\\n",
    "                                                \"POS\": \"genitive_marker\",\"PRP\": \"pronoun_personal\",\\\n",
    "                                                \"PRP$\": \"pronoun_possessive\",\\\n",
    "                                                \"RB\": \"adverb\",\"RBR\": \"adverb_comparative\",\\\n",
    "                                                \"RBS\": \"adverb_superlative\",\"RP\": \"particle\",\\\n",
    "                                                \"TO\":\"to_preposition\",\"UH\": \"interjection\",\\\n",
    "                                                \"VB\": \"verb_base\",\"VBD\":\"verb_past\",\\\n",
    "                                                \"VBG\":\"gerund\",\"VBP\": \"verb_present_3s\",\\\n",
    "                                                \"VBZ\": \"verb_present_3s\",\"WP\": \"WH_pronoun\",\n",
    "                                                \"WRB\": \"Wh_adverb\",\"WP$\":\"possessive_wh_pronoun\",\\\n",
    "                            \"FW\":\"foreign word\",\"SYM\":\"symbol\",'NNPS':\"proper_noun\"},inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239ec2d-0b41-4814-9194-6c984dc8365e",
   "metadata": {},
   "source": [
    "## Counting the number of verbs,nouns and pronouns from the part of speech data\n",
    "- This was done by adding the contents of the column that had verbs in it,that had nouns in it etc.\n",
    "- The part of speech features were calculated seperately for the train and test data and stored in seperate .csv files so that it not be calculated every single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786512e-3a72-460d-ae0d-fd86e3359ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_cols = [col for col in pos_counts_train_data.columns if 'verb' in col and \"adverb\" not in col]\n",
    "noun_cols=[col for col in pos_counts_train_data.columns if 'noun' in col and \"pronoun\" not in col]\n",
    "pronoun_cols=[col for col in pos_counts_train_data.columns if \"pronoun\" in col]\n",
    "pos_counts_train_data[\"verb_count\"]=pos_counts_train_data[verb_cols].sum(axis = 1, skipna = True)\n",
    "pos_counts_train_data[\"noun_count\"]=pos_counts_train_data[noun_cols].sum(axis = 1, skipna = True)\n",
    "pos_counts_train_data[\"pronoun_count\"]=pos_counts_train_data[pronoun_cols].sum(axis = 1, skipna = True)\n",
    "\n",
    "verb_cols = [col for col in pos_counts_test_data.columns if 'verb' in col and \"adverb\" not in col]\n",
    "noun_cols=[col for col in pos_counts_test_data.columns if 'noun' in col and \"pronoun\" not in col]\n",
    "pronoun_cols=[col for col in pos_counts_test_data.columns if \"pronoun\" in col]\n",
    "pos_counts_test_data[\"verb_count\"]=pos_counts_test_data[verb_cols].sum(axis = 1, skipna = True)\n",
    "pos_counts_test_data[\"noun_count\"]=pos_counts_test_data[noun_cols].sum(axis = 1, skipna = True)\n",
    "pos_counts_test_data[\"pronoun_count\"]=pos_counts_test_data[pronoun_cols].sum(axis = 1, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960da388-a4e5-4b81-87ea-a91f99b98703",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_train_data.to_csv(\"feature2/pos_tag_traindata.csv\",index=False)\n",
    "pos_counts_test_data.to_csv(\"feature2/pos_tag_testdata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e6d42-b0b5-42c8-a683-703f1a191b06",
   "metadata": {},
   "source": [
    "## Calculating features from the AoA file \n",
    "- From the AoA file two types of features were calculated,the aggregated features for all words in the sentence and count features.\n",
    "- For every input sample the number of words in common with the AoA file was calculated.\n",
    "\n",
    "### Aggregated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3409a-7e3f-432c-85e3-1e9c24b89f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AoA=pd.read_csv(\"AoA_51715_words.csv\")\n",
    "AoA=AoA.fillna(0)\n",
    "AoA.columns=AoA.columns.str.lower()\n",
    "words_in_AoA=set(AoA.word)\n",
    "AoA=AoA.set_index('word')\n",
    "words_in_aoa=[]\n",
    "num_in_aoa=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd1339-ac58-45ea-aead-b031ef87ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AOA_words(data):\n",
    "    words_in_aoa=[]\n",
    "    num_in_aoa=[]\n",
    "    for i in range(len(data)):\n",
    "        words=set(data.iloc[i][\"tokenized_text\"])\n",
    "        p=list(set(words) & set(words_in_AoA))\n",
    "        words_in_aoa.append(p)\n",
    "        num_in_aoa.append(len(p))\n",
    "        if(i%10000==0):       \n",
    "            print(\"10k done\")\n",
    "    data[\"aoa_words\"]=words_in_aoa\n",
    "    data[\"num_words_AoA\"]=num_in_aoa\n",
    "train_data=AOA_words(train_data)\n",
    "test_data=AOA_words(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357188a-07b7-4e38-ab75-bc1ef42d363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AOA_aggregated_features(data):\n",
    "    sentence_aoa=pd.DataFrame()\n",
    "    for i in range(len(data)):\n",
    "        word_aoa=pd.DataFrame()\n",
    "        for word in data.iloc[i][\"aoa_words\"]:\n",
    "            word_aoa=word_aoa.append(AoA.loc[word])\n",
    "            word_aoa=word_aoa[[\"aoa_bird_lem\",\"aoa_bristol_lem\",\"aoa_cort_lem\",\"aoa_kup\",\"aoa_kup_lem\",\"aoa_schock\",\"freq_pm\",\"nletters\",  \"nphon\",  \"nsyll\", \"perc_known\",\"perc_known_lem\"]]\n",
    "        if(i%1000==0):\n",
    "            print(\"1k done\")\n",
    "        sentence_aoa=sentence_aoa.append(word_aoa.mean(axis=0),ignore_index=True)\n",
    "    return(sentence_aoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6f348-f5b7-4f00-a5ee-4bba7d3457ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aoa_aggregated=AOA_aggregated_features(train_data)\n",
    "test_aoa_aggregated=AOA_aggregated_features(train_data)\n",
    "train_aoa_aggregated.to_csv(\"feature2/aoa_features_traindata.csv\",index=False)\n",
    "test_aoa_aggregated.to_csv(\"feature2/aoa_features_testdata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966fb13-7178-4a4d-b6e6-7a0d974c58d6",
   "metadata": {},
   "source": [
    "### Numerical Features\n",
    "- The number of words in the input sample which has \n",
    "    - less than 5 phenomes\n",
    "    - less than 10 phenomes\n",
    "    - less than 15 phenomes \n",
    "    - less than 20 phenomes\n",
    "- The number of words in the input sample for which the percentage known is\n",
    "    - less than 20%\n",
    "    - less than 50%\n",
    "    - greater than 90%\n",
    "    - greater than 75%\n",
    "- The number of words in the input sample for which the Estimated AoA based on Kuperman et al. study is\n",
    "    - less than 10\n",
    "    - less than 20\n",
    "    - greater than 20\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963465f-cf33-4a26-95e9-66e208b9d6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_less5_phonemes=[]     \n",
    "words_less10_phonemes=[]     \n",
    "words_less15_phonemes=[]     \n",
    "words_less20_phonemes=[]     \n",
    "perc_known_less_20=[]     \n",
    "perc_known_less_50=[]     \n",
    "perc_known_greater_90=[]     \n",
    "perc_known_greater_75=[]     \n",
    "aoa_kup_lem_less_10=[]     \n",
    "aoa_kup_lem_less_20=[]     \n",
    "aoa_kup_lem_greater_20=[]     \n",
    "words_less5_phonemes_AoA=set(AoA[AoA[\"nphon\"]<=5][\"word\"])\n",
    "words_less10_phonemes_AoA=set(AoA[(AoA[\"nphon\"]>5) &(AoA[\"nphon\"]<=10)][\"word\"])\n",
    "words_less15_phonemes_AoA=set(AoA[(AoA[\"nphon\"]>10) & AoA[\"nphon\"]<=15][\"word\"])\n",
    "words_less20_phonemes_AoA=set(AoA[(AoA[\"nphon\"]>15) & AoA[\"nphon\"]<=20][\"word\"])\n",
    "perc_known_less_20_AoA=set(AoA[AoA['perc_known']<=0.20][\"word\"])\n",
    "perc_known_less_50_AoA=set(AoA[AoA['perc_known']<=0.50][\"word\"])\n",
    "perc_known_greater_90_AoA=set(AoA[AoA['perc_known']>=0.90][\"word\"])\n",
    "perc_known_greater_75_AoA=set(AoA[AoA['perc_known']>=0.75][\"word\"])\n",
    "aoa_kup_lem_less_10_AoA=set(AoA[AoA['aoa_kup_lem']<=10][\"word\"])\n",
    "aoa_kup_lem_less_20_AoA=set(AoA[AoA['aoa_kup_lem']<=20][\"word\"])\n",
    "aoa_kup_lem_greater_20_AoA=set(AoA[AoA['aoa_kup_lem']>=20][\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306087b5-4a99-47e2-ab37-fd3ac27d5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa_count_features(data):\n",
    "    AoA_countfeatures_data=pd.DataFrame()\n",
    "    for i in range(len(data)):\n",
    "    words=set(data.iloc[i][\"aoa_words\"])\n",
    "    words_less5_phonemes.append(len(set(words) & words_less5_phonemes_AoA))\n",
    "    words_less10_phonemes.append(len(set(words) & words_less10_phonemes_AoA))\n",
    "    words_less15_phonemes.append(len(set(words) & words_less15_phonemes_AoA))\n",
    "    words_less20_phonemes.append(len(set(words) & words_less20_phonemes_AoA))\n",
    "    perc_known_less_20.append(len(set(words) & perc_known_less_20_AoA))\n",
    "    perc_known_less_50.append(len(set(words) & perc_known_less_50_AoA))\n",
    "    perc_known_greater_90.append(len(set(words) & perc_known_greater_90_AoA ))\n",
    "    perc_known_greater_75.append(len(set(words) & perc_known_greater_75_AoA ))\n",
    "    aoa_kup_lem_less_10.append(len(set(words) & aoa_kup_lem_less_10_AoA))\n",
    "    aoa_kup_lem_less_20.append(len(set(words) & aoa_kup_lem_less_20_AoA))\n",
    "    aoa_kup_lem_greater_20.append(len(set(words) & aoa_kup_lem_greater_20_AoA))\n",
    "    if(i%10000==0):       \n",
    "        print(\"10k done\")\n",
    "    AoA_countfeatures_data[\"words_less5_phonemes\"]=words_less5_phonemes\n",
    "    AoA_countfeatures_data[\"words_less10_phonemes\"]=words_less10_phonemes\n",
    "    AoA_countfeatures_data[\"words_less15_phonemes\"]=words_less15_phonemes\n",
    "    AoA_countfeatures_data[\"words_less20_phonemes\"]=words_less20_phonemes\n",
    "    AoA_countfeatures_data[\"perc_known_less_20\"]=perc_known_less_20\n",
    "    AoA_countfeatures_data[\"perc_known_less_50\"]=perc_known_less_50\n",
    "    AoA_countfeatures_data[\"perc_known_greater_90\"]=perc_known_greater_90\n",
    "    AoA_countfeatures_data[\"perc_known_greater_75\"]=perc_known_greater_75\n",
    "    AoA_countfeatures_data[\"aoa_kup_lem_less_10\"]=aoa_kup_lem_less_10\n",
    "    AoA_countfeatures_data[\"aoa_kup_lem_less_20\"]=aoa_kup_lem_less_20\n",
    "    AoA_countfeatures_data[\"aoa_kup_lem_greater_20\"]=aoa_kup_lem_greater_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f969c7-9e82-4cb3-91ea-b170ba111c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AoA_countfeatures_traindata=aoa_count_features(train_data)\n",
    "AoA_countfeatures_testdata=aoa_count_features(test_data)\n",
    "AoA_countfeatures_traindata.to_csv(\"AoA_countfeatures_traindata.csv\",index=False)\n",
    "AoA_countfeatures_testdata.to_csv(\"AoA_countfeatures_testdata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06957591-8490-4aa8-82f4-aefd6468a045",
   "metadata": {},
   "source": [
    "## Features calculated from concreteness\n",
    "- From the Concreteness_ratings_Brysbaert_et_al_BRM.txtfile two types of features were calculated,the aggregated features for all words in the sentence and count features.\n",
    "- For every input sample the number of words in common with the AoA file was calculated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27044e05-f023-49c0-838b-0de2d55ca681",
   "metadata": {},
   "outputs": [],
   "source": [
    "concreteness=pd.read_csv(\"Concreteness_ratings_Brysbaert_et_al_BRM.txt\",sep = '\\t')\n",
    "concreteness=concreteness.fillna(0)\n",
    "concreteness.columns=concreteness.columns.str.lower()\n",
    "wordsconcreteness=set(concreteness.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319315f3-9088-494b-8702-5ab2a26ea7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concreteness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bc021-91de-4dcc-86e8-a5883d74be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concreteness.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf55b8a-79c2-4c30-8661-427e718647b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_concreteness=[]\n",
    "num_in_concreteness=[]\n",
    "for i in range(len(train_data)):\n",
    "    words=set(train_data.iloc[i][\"tokenized_text\"])\n",
    "    p=list(set(words) & set(wordsconcreteness))\n",
    "    words_in_concreteness.append(p)\n",
    "    num_in_concreteness.append(len(p))\n",
    "    if(i%100000==0):       \n",
    "        print(\"100k done\")\n",
    "train_data[\"concreteness_words\"]=words_in_concreteness\n",
    "train_data[\"num_words_concreteness\"]=num_in_concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9535eb3-4887-42a2-91ba-75c00a94aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_concreteness=[]\n",
    "num_in_concreteness=[]\n",
    "for i in range(len(test_data)):\n",
    "    words=word_tokenize(test_data.iloc[i][\"original_text\"])\n",
    "    p=list(set(words) & set(wordsconcreteness))\n",
    "    words_in_concreteness.append(p)\n",
    "    num_in_concreteness.append(len(p))\n",
    "    if(i%100000==0):       \n",
    "        print(\"100k done\")\n",
    "test_data[\"concreteness_words\"]=words_in_concreteness\n",
    "test_data[\"num_words_concreteness\"]=num_in_concreteness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d5124-bec4-4053-ba87-a1501fa0b632",
   "metadata": {},
   "source": [
    "### Numerical Features\n",
    "\n",
    "- The number of words in the input sample which has \n",
    "    - less than 0.5 bigram\n",
    "    - greater than 0.5 bigram\n",
    "    - less than 15 phenomes \n",
    "    - less than 20 phenomes\n",
    "- The number of words in the input sample for which the mean concreteness rating is\n",
    "    - less than 2\n",
    "    - greater than 2\n",
    "- The number of words in the input sample for which the standard deviation of the concreteness ratings is\n",
    "    - less than 2.25\n",
    "    - greater than 2.25\n",
    "- The number of words in the input sample for which the number of people not knowing the word is\n",
    "    - less than 50\n",
    "    - greater than 50\n",
    "    - greater than 100\n",
    "- The number of words in the input sample for which the Percentage of participants who knew the word is\n",
    "    - less than 95%\n",
    "    - greater than 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdd132-dee8-40f1-bc9a-f8450452a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_less_point5_bigram=[]\n",
    "words_greater_point5_bigram=[]\n",
    "words_less2_concm=[]\n",
    "words_greater2_concm=[]     \n",
    "words_less125_consd=[]     \n",
    "words_great125_consd=[] \n",
    "words_great100_unknown=[]    \n",
    "words_less100_unknown=[]    \n",
    "total_less50=[]\n",
    "total_less100=[]\n",
    "total_great100=[]\n",
    "percent_knownless95=[]\n",
    "percent_knowngreater95=[]\n",
    "\n",
    "\n",
    "conc_words_less_point5_bigram=set(wordsconcreteness & set(concreteness[concreteness[\"bigram\"]<0.5][\"word\"]))\n",
    "conc_words_greater_point5_bigram=set(wordsconcreteness & set(concreteness[concreteness[\"bigram\"]>=0.5][\"word\"]))\n",
    "conc_words_less2_concm=set(wordsconcreteness & set(concreteness[concreteness[\"conc.m\"]<2][\"word\"]))\n",
    "conc_words_greater2_concm=set(wordsconcreteness & set(concreteness[concreteness[\"conc.m\"]>=2][\"word\"]) )    \n",
    "conc_words_less125_consd=set(wordsconcreteness & set(concreteness[concreteness[\"conc.sd\"]<2.25][\"word\"]))     \n",
    "conc_words_great125_consd=set(wordsconcreteness & set(concreteness[concreteness[\"conc.sd\"]>1.25][\"word\"])) \n",
    "conc_words_great100_unknown=set(wordsconcreteness & set(concreteness[concreteness[\"unknown\"]>=100][\"word\"]))    \n",
    "conc_words_less100_unknown=set(wordsconcreteness & set(concreteness[concreteness[\"unknown\"]<100][\"word\"]))    \n",
    "conc_total_less50=set(wordsconcreteness & set(concreteness[concreteness[\"total\"]<=50][\"word\"]))\n",
    "conc_total_less100=set(wordsconcreteness & set(concreteness[concreteness[\"total\"]<100][\"word\"]))\n",
    "conc_total_great100=set(wordsconcreteness & set(concreteness[concreteness[\"total\"]>=100][\"word\"]))\n",
    "conc_percent_knownless95=set(wordsconcreteness & set(concreteness[concreteness[\"percent_known\"]<0.95][\"word\"]))\n",
    "conc_percent_knowngreater95=set(wordsconcreteness & set(concreteness[concreteness[\"percent_known\"]>=0.95][\"word\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a15b5-7965-493c-a845-39b870291a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conc_numerical(data):\n",
    "    conc_countfeatures_data=pd.DataFrame()\n",
    "    for i in range(len(data)):\n",
    "        words=set(data.iloc[i][\"concreteness_words\"])\n",
    "        words_less_point5_bigram.append(len(set(words)&conc_words_less_point5_bigram))\n",
    "        words_greater_point5_bigram.append(len(set(words)&conc_words_greater_point5_bigram))\n",
    "        words_less2_concm.append(len(set(words)&conc_words_less2_concm))\n",
    "        words_greater2_concm.append(len(set(words)&conc_words_greater2_concm))     \n",
    "        words_less125_consd.append(len(set(words)&conc_words_less125_consd))     \n",
    "        words_great125_consd.append(len(set(words)&conc_words_great125_consd)) \n",
    "        words_great100_unknown.append(len(set(words)&conc_words_great100_unknown))    \n",
    "        words_less100_unknown.append(len(set(words)&conc_words_less100_unknown))    \n",
    "        total_less50.append(len(set(words)&conc_total_less50))\n",
    "        total_less100.append(len(set(words)&conc_total_less100))\n",
    "        total_great100.append(len(set(words)&conc_total_great100))\n",
    "        percent_knownless95.append(len(set(words)&conc_percent_knownless95))\n",
    "        percent_knowngreater95.append(len(set(words)&conc_percent_knowngreater95))\n",
    "        if(i%10000==0):       \n",
    "            print(\"10k done\")\n",
    "    conc_countfeatures_data[\"words_less_point5_bigram\"]=words_less_point5_bigram\n",
    "    conc_countfeatures_data[\"words_greater_point5_bigram\"]= words_greater_point5_bigram\n",
    "    conc_countfeatures_data[\"words_less2_concm\"]=words_less2_concm\n",
    "    # conc_countfeatures_ata[\"words_less20_phonemes\"]=words_less20_phonemes\n",
    "    conc_countfeatures_data[\"words_greater2_concm\"]=words_greater2_concm\n",
    "    conc_countfeatures_data[\"words_less125_consd\"]=words_less125_consd\n",
    "    conc_countfeatures_data[\"words_great125_consd\"]=words_great125_consd\n",
    "    conc_countfeatures_data[\"words_great100_unknown\"]=words_great100_unknown\n",
    "    conc_countfeatures_data[\"words_less100_unknown\"]=words_less100_unknown\n",
    "    conc_countfeatures_data[\"total_less50\"]=total_less50\n",
    "    conc_countfeatures_data[\"total_less100\"]=total_less100\n",
    "    conc_countfeatures_data[\"total_great100\"]=total_great100\n",
    "    conc_countfeatures_data[\"percent_knownless95\"]=percent_knownless95\n",
    "    conc_countfeatures_data[\"percent_knowngreater95\"]=percent_knowngreater95\n",
    "    return conc_countfeatures_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322b91f-24b4-43f5-ae01-b2ca72517e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_countfeatures_traindata=conc_numerical(train_data)\n",
    "conc_countfeatures_testdata=conc_numerical(test_data)\n",
    "conc_countfeatures_traindata.to_csv(\"feature2/concreteness_numerical_features_traindata.csv\",index=False)\n",
    "conc_countfeatures_testdata.to_csv(\"feature2/concreteness_numerical_features_testdata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9b1f0-d36f-476b-a379-dd6b80c75628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['tokenized_text',\"concreteness_words\",'aoa_words','pos_tokens']].to_csv(\"feature2/words_traindata.csv\",index=False)\n",
    "test_data[['tokenized_text',\"concreteness_words\",'aoa_words','pos_tokens']].to_csv(\"feature2/words_testdata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad1124-2414-42ad-9c54-c2ce2aaef3e9",
   "metadata": {},
   "source": [
    "### Aggregated Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d0e8e-9dff-4692-b5c0-5b1fa66007d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_concreteness=pd.DataFrame()\n",
    "for i in range(len(train_data)):\n",
    "    word_concreteness=pd.DataFrame()\n",
    "    for word in train_data.iloc[i][\"concreteness_words\"]:\n",
    "        word_concreteness=word_concreteness.append(concreteness.loc[word])\n",
    "        word_concreteness=word_concreteness[[\"bigram\",\"conc.m\",\"conc.sd\",\"unknown\",\"total\",\"percent_known\",\"subtlex\",\"dom_pos\"]]\n",
    "    if(i%10000==0):\n",
    "        print(\"10k done\")\n",
    "    sentence_concreteness=sentence_concreteness.append(word_concreteness.mean(axis=0),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18229e20-1abf-4960-901f-f9321c2976a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_concreteness.to_csv(\"feature2/concreteness_features_traindata.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a695e4-8729-4c67-87c3-a29d957780fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_concreteness=pd.DataFrame()\n",
    "for i in range(len(test_data)):\n",
    "    word_concreteness=pd.DataFrame()\n",
    "    for word in test_data.iloc[i][\"concreteness_words\"]:\n",
    "        word_concreteness=word_concreteness.append(concreteness.loc[word])\n",
    "        word_concreteness=word_concreteness[[\"bigram\",\"conc.m\",\"conc.sd\",\"unknown\",\"total\",\"percent_known\",\"subtlex\",\"dom_pos\"]]\n",
    "    sentence_concreteness=sentence_concreteness.append(word_concreteness.mean(axis=0),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec82514-3291-440a-be1f-63cd8512cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_concreteness.to_csv(\"feature2/concreteness_features_testdata.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
